<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>VOT Session</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      background-color: #f9f9f9;
      padding-top: 20px;
    }
    #patientInfo {
      font-size: 20px;
      margin-bottom: 10px;
    }
    /* Container for video, overlay, and step image */
    #videoContainer {
      position: relative; /* Enables overlay positioning */
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 20px;
      margin-bottom: 20px;
    }
    /* Webcam video styling: using object-fit: contain to display full video without cropping */
    #webcam {
      border: 2px solid #333;
      border-radius: 5px;
      width: 320px;
      height: 240px;
      object-fit: contain;
      background-color: black;
    }
    /* Overlay canvas positioned on top of the video */
    #overlay {
      position: absolute;
      left: 0;
      top: 0;
      pointer-events: none; /* Allow clicks to pass through */
    }
    /* Step image styling with fade transition */
    #stepImage {
      width: 320px;
      height: auto;
      border: 2px solid #333;
      border-radius: 5px;
      opacity: 0;
      transition: opacity 1s ease-in-out;
    }
    #prompt {
      font-size: 28px;
      font-weight: bold;
      margin-top: 20px;
      color: #333;
      transition: background-color 0.3s ease;
      padding: 10px;
      border-radius: 5px;
    }
    #startButton {
      padding: 10px 20px;
      font-size: 18px;
      margin: 20px auto;
      display: block;
      cursor: pointer;
      background-color: #4CAF50;
      color: white;
      border: none;
      border-radius: 5px;
    }
    #startButton:hover {
      background-color: #45a049;
    }
    /* Progress Bar Styles */
    #progressContainer {
      width: 80%;
      height: 20px;
      background-color: #e0e0e0;
      border-radius: 10px;
      margin: 20px auto;
      overflow: hidden;
    }
    #progressBar {
      height: 100%;
      width: 0%;
      background-color: #4CAF50;
      border-radius: 10px;
      transition: width 0.5s ease;
    }
    /* Steps List */
    #stepsList {
      width: 80%;
      margin: 20px auto;
      text-align: left;
    }
    .step {
      font-size: 20px;
      padding: 8px;
      border-bottom: 1px solid #ddd;
    }
    .completed {
      background-color: #d4edda;
    }
    .completed::before {
      content: "âœ“ ";
      color: green;
      font-weight: bold;
    }
    /* Success Modal */
    #successModal {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0,0,0,0.6);
      display: none;
      align-items: center;
      justify-content: center;
    }
    #modalContent {
      background: white;
      padding: 20px 30px;
      border-radius: 10px;
      text-align: center;
    }
    #modalContent h2 {
      margin-top: 0;
    }
    #modalContent button {
      padding: 10px 20px;
      font-size: 16px;
      cursor: pointer;
      background-color: #4CAF50;
      color: white;
      border: none;
      border-radius: 5px;
      margin-top: 20px;
    }
    #modalContent button:hover {
      background-color: #45a049;
    }
  </style>
  <!-- Load TensorFlow.js and BlazeFace Model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
</head>
<body>
  <h1>VOT Live Session</h1>
  <!-- Display Patient ID -->
  <p id="patientInfo"></p>
  
  <!-- Container for video, overlay, and step image -->
  <div id="videoContainer">
    <video id="webcam" autoplay playsinline></video>
    <!-- Overlay canvas for tracking -->
    <canvas id="overlay"></canvas>
    <img id="stepImage" src="" alt="Step Image">
  </div>
  
  <!-- Progress Bar -->
  <div id="progressContainer">
    <div id="progressBar"></div>
  </div>
  
  <!-- Prompt Display -->
  <div id="prompt"></div>
  
  <!-- Steps List -->
  <div id="stepsList"></div>
  
  <!-- Start Button -->
  <button id="startButton" onclick="startVerification()">Start VOT</button>
  
  <!-- Success Modal -->
  <div id="successModal">
    <div id="modalContent">
      <h2>VOT Successful!</h2>
      <p>Your session has been completed successfully.</p>
      <button onclick="closeModal()">OK</button>
    </div>
  </div>
  
  <script>
    // Utility Function: Extract a query parameter value from the URL.
    function getQueryParam(param) {
      const urlParams = new URLSearchParams(window.location.search);
      return urlParams.get(param);
    }
    
    // Retrieve the patientID from the URL (e.g., vot.html?patientID=12345)
    const patientID = getQueryParam('patientID');
    document.getElementById('patientInfo').innerText = "Patient ID: " + patientID;
    
    /***********************
     * Global Variables for Tracking Overlay
     ***********************/
    let faceModel;  // BlazeFace model for face detection
    const overlayCanvas = document.getElementById('overlay');
    const overlayCtx = overlayCanvas.getContext('2d');
    
    // Resize the overlay canvas to match the displayed video dimensions using getBoundingClientRect
    function resizeOverlay() {
      const video = document.getElementById('webcam');
      const rect = video.getBoundingClientRect();
      overlayCanvas.width = rect.width;
      overlayCanvas.height = rect.height;
    }
    
    // Start face tracking using BlazeFace
    async function startTracking() {
      try {
        if (!faceModel) {
          faceModel = await blazeface.load();
        }
        const video = document.getElementById('webcam');
        resizeOverlay();
        requestAnimationFrame(() => trackFrame(video));
      } catch (error) {
        console.error("Tracking error:", error);
      }
    }
    
    // Process each frame for face detection and draw the bounding box with scaling, letterboxing offsets, and fixed offset
    async function trackFrame(video) {
      if (video.readyState === 4) {
        const predictions = await faceModel.estimateFaces(video, false);
        overlayCtx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
        
        // Get displayed dimensions using getBoundingClientRect
        const rect = video.getBoundingClientRect();
        const containerWidth = rect.width;
        const containerHeight = rect.height;
        
        // Get intrinsic video dimensions
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;
        
        // Calculate scale factor and letterboxing offsets
        const scale = Math.min(containerWidth / videoWidth, containerHeight / videoHeight);
        const displayedWidth = videoWidth * scale;
        const displayedHeight = videoHeight * scale;
        const offsetX = (containerWidth - displayedWidth) / 2;
        const offsetY = (containerHeight - displayedHeight) / 2;
        
        // Fixed offset to push the box further right (in pixels)
        const fixedOffset = 150; // Increased from 80 to 150
        
        if (predictions.length > 0) {
          predictions.forEach(prediction => {
            const start = prediction.topLeft;
            const end = prediction.bottomRight;
            const boxWidth = (end[0] - start[0]) * scale;
            const boxHeight = (end[1] - start[1]) * scale;
            const startX = start[0] * scale + offsetX + fixedOffset;
            const startY = start[1] * scale + offsetY;
            
            overlayCtx.strokeStyle = "red";
            overlayCtx.lineWidth = 3;
            overlayCtx.beginPath();
            overlayCtx.rect(startX, startY, boxWidth, boxHeight);
            overlayCtx.stroke();
          });
        }
      }
      requestAnimationFrame(() => trackFrame(video));
    }
    
    /***********************
     * VOT Session Code
     ***********************/
    let currentStep = 0;
    const steps = [
      { prompt: "Show your face in the camera", class: 0 },
      { prompt: "Show the pill in your hand", class: 1 },
      { prompt: "Put the pill in your mouth", class: 2 },
      { prompt: "Open mouth & show tongue to show it's empty", class: 3 }
    ];
    
    // Image filenames corresponding to each step
    const stepImages = [
      "face.jpg",
      "pill_in_hand.jpg",
      "pill_in_mouth.jpg",
      "mouth_empty.jpg"
    ];
    
    function initializeStepsList() {
      const stepsList = document.getElementById('stepsList');
      stepsList.innerHTML = "";
      steps.forEach((step, index) => {
        const stepDiv = document.createElement("div");
        stepDiv.className = "step";
        stepDiv.id = "step" + index;
        stepDiv.innerText = step.prompt;
        stepsList.appendChild(stepDiv);
      });
    }
    
    function updateProgressBar() {
      const progressBar = document.getElementById('progressBar');
      const percentage = (currentStep / steps.length) * 100;
      progressBar.style.width = percentage + "%";
    }
    
    function updateStepImage(step) {
      const stepImage = document.getElementById('stepImage');
      stepImage.style.opacity = 0;
      setTimeout(() => {
        stepImage.src = stepImages[step];
        stepImage.style.opacity = 1;
      }, 500);
    }
    
    async function startVerification() {
      try {
        document.getElementById('startButton').style.display = "none";
        initializeStepsList();
    
        // Load the VOT classification model (your Teachable Machine model)
        const model = await tf.loadLayersModel('https://teachablemachine.withgoogle.com/models/sNq4XZQSz/model.json');
    
        // Access the webcam
        const webcam = document.getElementById('webcam');
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        webcam.srcObject = stream;
    
        webcam.onloadedmetadata = () => {
          resizeOverlay();
          startTracking();
        };
    
        // Display the first prompt and step image
        document.getElementById('prompt').innerText = steps[currentStep].prompt;
        updateStepImage(currentStep);
        updateProgressBar();
    
        // Start checking the webcam feed every 500ms for VOT step predictions
        setInterval(() => verifyStep(model), 500);
      } catch (error) {
        alert("Error: " + error.message);
      }
    }
    
    async function verifyStep(model) {
      const webcam = document.getElementById('webcam');
      const image = tf.browser.fromPixels(webcam)
                      .resizeBilinear([224, 224])
                      .toFloat()
                      .div(255.0)
                      .expandDims();
    
      const prediction = await model.predict(image);
      const predictedClass = prediction.argMax(1).dataSync()[0];
      const confidence = prediction.max().dataSync()[0];
    
      console.log("---");
      console.log("Current Step:", currentStep, "Expected Class:", steps[currentStep].class);
      console.log("Predicted Class:", predictedClass, "Confidence:", confidence);
    
      if (confidence > 0.9 && steps[currentStep].class === predictedClass) {
        const stepElement = document.getElementById("step" + currentStep);
        stepElement.classList.add("completed");
    
        currentStep++;
        updateProgressBar();
    
        if (currentStep === steps.length) {
          document.getElementById('successModal').style.display = "flex";
        } else {
          document.getElementById('prompt').innerText = steps[currentStep].prompt;
          updateStepImage(currentStep);
        }
      }
    }
    
    function closeModal() {
      document.getElementById('successModal').style.display = "none";
      logSession("Success", "");
    }
    
    async function logSession(outcome, failureReason) {
      const data = {
        patientID: patientID,
        outcome: outcome,
        failureReason: failureReason
      };
    
      try {
        const response = await fetch("https://script.google.com/macros/s/AKfycbw5JFBoAvXweHxdeMb330vaexFAUlqZoG6-sLjuzyZwou4-2G5N1fYmTwCyjVhaN7ln/exec", {
          method: "POST",
          mode: "no-cors",
          headers: {
            "Content-Type": "application/json"
          },
          body: JSON.stringify(data)
        });
        console.log("Session logged:", data);
      } catch (error) {
        console.error("Error logging session:", error);
      }
    }
  </script>
</body>
</html>
